Setting Up the Environment

To begin, ensure you have the necessary dependencies installed. You can install the required Python packages using the following commands:

pip install fastapi uvicorn openai

Also, obtain your OpenAI API key.
Backend Development with FastAPI and OpenAI

The application is built using FastAPI, a modern, fast web framework for building APIs with Python. Additionally, it integrates the GPT-3.5 Turbo model from OpenAI to generate responses for user queries.

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request
from fastapi.responses import FileResponse
import openai
import time
import os

FastAPI Configuration

The FastAPI application is created, and the necessary environment variables are set, including the OpenAI API key.

app = FastAPI()
os.environ["OPENAI_API_KEY"] = "Your_API_key"
client = openai.OpenAI()
start_time = time.time()

Connection Manager

A ConnectionManager class is defined to manage active WebSocket connections.

class ConnectionManager:
    def __init__(self):
        self.active_connections = []
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    async def send_text(self, text: str, websocket: WebSocket):
        await websocket.send_text(text)
manager = ConnectionManager()

WebSocket Endpoint

The main WebSocket endpoint is defined to handle real-time communication with clients.

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            try:
                # Receive text data (speech recognition result) from the client
                data = await websocket.receive_text()
                
                # Process the data
                print(f"Received text: {data}")  # Example: print it to the console
                res = call_open_api(data)
                # Optionally, send a response back to the client
                collected_chunks = []
                collected_messages = []
                # iterate through the stream of events
                for chunk in res:
                    chunk_time = time.time() - start_time  # calculate the time delay of the chunk
                    collected_chunks.append(chunk)  # save the event response
                    chunk_message = chunk.choices[0].delta.content  # extract the message
                    collected_messages.append(chunk_message)  # save the message
                    
                    
                    if chunk_message is not None and chunk_message.find('.') != -1:
                        print("Found full stop")
                        message = [m for m in collected_messages if m is not None]
                        full_reply_content = ''.join([m for m in message])

                        await manager.send_text(full_reply_content, websocket)
                        collected_messages = []

                    print(f"Message received {chunk_time:.2f} seconds after request: {chunk_message}")  # print the delay and text

                if len(collected_messages) > 0:
                    message = [m for m in collected_messages if m is not None]
                    full_reply_content = ''.join([m for m in message])

                    await manager.send_text(full_reply_content, websocket)
                    collected_messages = []
                
            except WebSocketDisconnect:
                manager.disconnect(websocket)
                break
            except Exception as e:
                # Handle other exceptions
                print(f"Error: {str(e)}")
                break
    finally:
        manager.disconnect(websocket)

Serving HTML Page

An additional API endpoint is provided to serve an HTML page for client interaction.

@app.get("/")
async def get():
    return FileResponse("voice_frontend.html")

HTML Structure

The HTML structure defines the layout of the web page, including a title, a “Start Continuous Recognition” button, and a status indicator.

<!DOCTYPE html>
<html>
<head>
    <title>Real-time Speech Recognition and Text-to-Speech</title>
    <style>
        /* Styles for the page */
    </style>
</head>
<body>
    <h1>Audio-only Chat</h1>
    <button id="startButton">Start Continuous Recognition</button>
    <p id="status"></p>
<!-- JavaScript code for speech recognition and WebSocket communication -->
    <script>
        // JavaScript code
    </script>
</body>
</html>

JavaScript Code

The JavaScript code defines the logic for speech recognition and WebSocket communication. It utilizes the Web Speech API for speech recognition and text-to-speech functionality. The WebSocket connection is established to communicate with the FastAPI server.

const startButton = document.getElementById('startButton');
const status = document.getElementById('status');
let ws;
let recognition;

function startRecognition() {
    if ('webkitSpeechRecognition' in window) {
        recognition = new webkitSpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = false;

        recognition.onstart = () => {
            status.innerText = 'Speech recognition is on. Speak into the microphone.';
        };

        recognition.onresult = (event) => {
            let transcript = event.results[event.resultIndex][0].transcript;
            // Stop text-to-speech
            window.speechSynthesis.cancel();
            ws.send(transcript);
        };

        recognition.onerror = (event) => {
            status.innerText = 'Speech recognition error: ' + event.error;
        };

        recognition.onend = () => {
            recognition.start();
        };

        recognition.start();
    } else {
        status.innerText = 'Your browser does not support Web Speech API.';
    }
}

function speakText(text) {
    let speech = new SpeechSynthesisUtterance(text);
    window.speechSynthesis.speak(speech);
}

startButton.onclick = () => {
    ws = new WebSocket('wss://127.0.0.1:8000/ws');
    ws.onopen = () => {
        startRecognition();
    };
    ws.onmessage = (event) => {
        speakText(event.data);
    };
    ws.onerror = (event) => {
        console.error('WebSocket error:', event);
    };
    ws.onclose = () => {
        recognition.stop();
        status.innerText = 'WebSocket disconnected.';
    };
};



https://www.google.com/intl/en/chrome/demos/speech.html